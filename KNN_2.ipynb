{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e0e7ce6",
   "metadata": {},
   "source": [
    "Q1. What is the main difference between the Euclidean distance metric and the Manhattan distance metric in KNN? How might this difference affect the performance of a KNN classifier or regressor?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa230c6",
   "metadata": {},
   "source": [
    "The main difference lies in how distance is calculated between points:\n",
    "\n",
    "\n",
    "* Euclidean Distance: Measures the straight-line distance between two points.\n",
    "\n",
    "* Manhattan Distance: Also known as L1 norm or taxicab distance, measures the distance as the sum of the absolute differences between the coordinates of the points along the axes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2846d31b",
   "metadata": {},
   "source": [
    "Q2. How do you choose the optimal value of k for a KNN classifier or regressor? What techniques can be used to determine the optimal k value?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b56f5c",
   "metadata": {},
   "source": [
    "The optimal value of k is often chosen through techniques like cross-validation. You can split the data into training and validation sets, train the model with different values of k, and choose the one that gives the best performance on the validation set. Other methods include grid search or random search over a range of k values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8888f298",
   "metadata": {},
   "source": [
    "Q3. How does the choice of distance metric affect the performance of a KNN classifier or regressor? In what situations might you choose one distance metric over the other?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfae9f3",
   "metadata": {},
   "source": [
    "The choice of distance metric depends on the nature of the data. Euclidean distance is suitable when the relationships between features are more complex and when the data is not on a grid-like structure. Manhattan distance might be preferable when dimensions represent different units or when movement is restricted to grid lines, as in city-block navigation. Experimenting with both metrics and selecting the one that performs better on the specific dataset is recommended."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6caf007c",
   "metadata": {},
   "source": [
    "Q4. What are some common hyperparameters in KNN classifiers and regressors, and how do they affect the performance of the model? How might you go about tuning these hyperparameters to improve model performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93351ac0",
   "metadata": {},
   "source": [
    "Common hyperparameters include k (number of neighbors), the choice of distance metric, and sometimes the weighting of neighbors. The number of neighbors influences the model's bias-variance trade-off. Tuning involves experimenting with different values of these hyperparameters and evaluating the model's performance using techniques like grid search or random search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd53706",
   "metadata": {},
   "source": [
    "Q5. How does the size of the training set affect the performance of a KNN classifier or regressor? What techniques can be used to optimize the size of the training set?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea33e04e",
   "metadata": {},
   "source": [
    "A larger training set can provide more representative information about the data distribution and improve the generalization of the model. However, too large a dataset can lead to increased computational costs. The optimal size depends on the complexity of the problem and the characteristics of the data. Techniques like cross-validation can help determine an appropriate training set size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecaaf399",
   "metadata": {},
   "source": [
    "Q6. What are some potential drawbacks of using KNN as a classifier or regressor? How might you overcome these drawbacks to improve the performance of the model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed0c851",
   "metadata": {},
   "source": [
    "Drawbacks include sensitivity to irrelevant features, computational cost for large datasets, and susceptibility to the curse of dimensionality. Feature selection or dimensionality reduction can address the first issue. Efficient data structures like KD-trees can mitigate the computational cost. To tackle the curse of dimensionality, feature engineering or employing dimensionality reduction techniques can be beneficial. Additionally, scaling features appropriately and handling missing values can improve KNN's robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8455436",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
